{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c49b768",
   "metadata": {},
   "source": [
    "## **NLP A1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c63218",
   "metadata": {},
   "source": [
    "### **1.2 Modify the Word2Vec (with & without negative sampling) and GloVe from the lab lecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f0571fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all modules\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59774178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to C:\\Users\\Legion 5\n",
      "[nltk_data]     Pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Legion 5\n",
      "[nltk_data]     Pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Legion 5\n",
      "[nltk_data]     Pro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data (‡∏£‡∏±‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å)\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "466f44cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10007 sentences\n"
     ]
    }
   ],
   "source": [
    "# 1. LOAD REAL CORPUS (Reuters News)\n",
    "\n",
    "def load_reuters_corpus(max_sents=10000):\n",
    "    \"\"\"Load Reuters news as List[List[str]]\"\"\"\n",
    "    sents = []\n",
    "    for fileid in reuters.fileids():\n",
    "        for sent in reuters.sents(fileid):\n",
    "            tokens = [w.lower() for w in sent if w.isalpha() and len(w) >= 2]\n",
    "            if len(tokens) >= 3:\n",
    "                sents.append(tokens)\n",
    "        if max_sents and len(sents) >= max_sents:\n",
    "            break\n",
    "    return sents\n",
    "\n",
    "corpus = load_reuters_corpus(max_sents=10000)\n",
    "print(f\"Loaded {len(corpus)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b550659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 12650\n"
     ]
    }
   ],
   "source": [
    "# 2. VOCAB + WORD2INDEX (‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏° Word2Vec/GloVe)\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocab = list(set(flatten(corpus)))\n",
    "vocab.append('UNK')\n",
    "vocab = vocab[:20000]  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î vocab ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏£‡πá‡∏ß\n",
    "\n",
    "word2index = {w: i for i, w in enumerate(vocab)}\n",
    "index2word = {i: w for w, i in word2index.items()}\n",
    "voc_size = len(vocab)\n",
    "print(f\"Vocab size: {voc_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77bfb808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. WORD2VEC FUNCTIONS (Dynamic Window)\n",
    "\n",
    "def build_skipgrams(corpus, word2index, window_size=2):\n",
    "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á skip-gram pairs ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏£‡∏±‡∏ö window_size\"\"\"\n",
    "    pairs = []\n",
    "    for sent in corpus:\n",
    "        indices = [word2index.get(w, word2index['UNK']) for w in sent]\n",
    "        for i, center in enumerate(indices):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(indices), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if j != i:\n",
    "                    pairs.append((center, indices[j]))\n",
    "    return pairs\n",
    "\n",
    "def randombatch_with_neg(batch_size, skipgrams, neg_probs, num_negatives=5):\n",
    "    \"\"\"‡∏™‡∏∏‡πà‡∏° batch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö negative sampling\"\"\"\n",
    "    indices = np.random.choice(len(skipgrams), batch_size, replace=False)\n",
    "    centers, positives, negatives = [], [], []\n",
    "    for idx in indices:\n",
    "        c, p = skipgrams[idx]\n",
    "        centers.append(c)\n",
    "        positives.append(p)\n",
    "        negs = np.random.choice(len(neg_probs), size=num_negatives, p=neg_probs)\n",
    "        negatives.append(negs)\n",
    "    return (\n",
    "        torch.from_numpy(np.array(centers, dtype=np.int64)),\n",
    "        torch.from_numpy(np.array(positives, dtype=np.int64)),\n",
    "        torch.from_numpy(np.array(negatives, dtype=np.int64))\n",
    "    )\n",
    "\n",
    "# Negative sampling distribution\n",
    "word_counts = np.zeros(voc_size)\n",
    "for sent in corpus:\n",
    "    for w in sent:\n",
    "        idx = word2index.get(w, word2index['UNK'])\n",
    "        word_counts[idx] += 1\n",
    "neg_sampling_probs = word_counts ** 0.75 / (word_counts ** 0.75).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aede1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. GLOVE FUNCTIONS (Dynamic Window)\n",
    "\n",
    "def build_cooccurrence(corpus, window_size=2):\n",
    "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á co-occurrence matrix ‡∏£‡∏±‡∏ö window_size\"\"\"\n",
    "    cooc = Counter()\n",
    "    for sent in corpus:\n",
    "        for i, w in enumerate(sent):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sent), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if j != i:\n",
    "                    cooc[(w, sent[j])] += 1\n",
    "    return cooc\n",
    "\n",
    "def weighting(wi, wj, Xik, xmax=100, alpha=0.75):\n",
    "    \"\"\"GloVe weighting function\"\"\"\n",
    "    x = Xik.get((wi, wj), 1)\n",
    "    if x < xmax:\n",
    "        return (x / xmax) ** alpha\n",
    "    return 1.0\n",
    "\n",
    "def randombatch_glove(batch_size, cooc, weightingdic):\n",
    "    \"\"\"‡∏™‡∏∏‡πà‡∏° batch ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GloVe\"\"\"\n",
    "    pairs = list(cooc.keys())\n",
    "    indices = np.random.choice(len(pairs), batch_size, replace=False)\n",
    "    inputs, targets, coocs, weightings = [], [], [], []\n",
    "    for idx in indices:\n",
    "        wi, wj = pairs[idx]\n",
    "        i, j = word2index.get(wi, 0), word2index.get(wj, 0)\n",
    "        inputs.append(i)\n",
    "        targets.append(j)\n",
    "        coocs.append(np.log(cooc[(wi, wj)]))\n",
    "        weightings.append(weighting(wi, wj, cooc))\n",
    "    return (\n",
    "        np.array(inputs, dtype=np.int64),\n",
    "        np.array(targets, dtype=np.int64),\n",
    "        np.array(coocs, dtype=np.float32),\n",
    "        np.array(weightings, dtype=np.float32)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ee7ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. MODELS\n",
    "\n",
    "class SkipgramNegSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.v_embed = nn.Embedding(vocab_size, emb_size)\n",
    "        self.u_embed = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "    def forward(self, center_words, pos_words, neg_words):\n",
    "        batch_size = center_words.size(0)\n",
    "        v = self.v_embed(center_words)\n",
    "        u_pos = self.u_embed(pos_words)\n",
    "        u_neg = self.u_embed(neg_words)\n",
    "\n",
    "        pos_score = torch.sum(v * u_pos, dim=1)\n",
    "        neg_score = torch.bmm(u_neg, v.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        pos_loss = F.logsigmoid(pos_score)\n",
    "        neg_loss = F.logsigmoid(-neg_score).sum(1)\n",
    "        loss = -(pos_loss + neg_loss).mean()\n",
    "        return loss\n",
    "\n",
    "class GloVeModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.v_embed = nn.Embedding(vocab_size, emb_size)\n",
    "        self.u_embed = nn.Embedding(vocab_size, emb_size)\n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        v = self.v_embed(center_words)\n",
    "        u = self.u_embed(target_words)\n",
    "        v_bias = self.v_bias(center_words).squeeze(1)\n",
    "        u_bias = self.u_bias(target_words).squeeze(1)\n",
    "\n",
    "        inner_prod = torch.sum(v * u, dim=1)\n",
    "        loss = weighting * (inner_prod + v_bias + u_bias - coocs) ** 2\n",
    "        return loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bad6c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgrams: 868334 pairs\n",
      "Training Word2Vec (Negative Sampling)...\n",
      "Epoch 500, loss = 20.1012\n",
      "Epoch 1000, loss = 19.4003\n",
      "Epoch 1500, loss = 17.9985\n",
      "Epoch 2000, loss = 15.3495\n"
     ]
    }
   ],
   "source": [
    "# 6. TRAINING (Word2Vec NS)\n",
    "\n",
    "window_size = 2\n",
    "skipgrams = build_skipgrams(corpus, word2index, window_size)\n",
    "print(f\"Skipgrams: {len(skipgrams)} pairs\")\n",
    "\n",
    "emb_size = 100\n",
    "batch_size = 256\n",
    "num_epochs = 2000\n",
    "\n",
    "model_ns = SkipgramNegSampling(voc_size, emb_size).to(device)\n",
    "optimizer_ns = optim.Adam(model_ns.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training Word2Vec (Negative Sampling)...\")\n",
    "for epoch in range(num_epochs):\n",
    "    centers, positives, negatives = randombatch_with_neg(\n",
    "        batch_size, skipgrams, neg_sampling_probs, num_negatives=5\n",
    "    )\n",
    "    centers = centers.to(device)\n",
    "    positives = positives.to(device)\n",
    "    negatives = negatives.to(device)\n",
    "\n",
    "    optimizer_ns.zero_grad()\n",
    "    loss = model_ns(centers, positives, negatives)\n",
    "    loss.backward()\n",
    "    optimizer_ns.step()\n",
    "\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1}, loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4151cb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence pairs: 344434\n",
      "Training GloVe...\n",
      "GloVe Epoch 500, loss = 7011.0259\n",
      "GloVe Epoch 1000, loss = 5438.5010\n",
      "GloVe Epoch 1500, loss = 3306.8403\n",
      "GloVe Epoch 2000, loss = 3165.7822\n",
      "GloVe Epoch 2500, loss = 3275.2668\n",
      "Training completed! Both Word2Vec (NS) and GloVe ready.\n"
     ]
    }
   ],
   "source": [
    "# 7. TRAINING (GloVe)\n",
    "\n",
    "cooc = build_cooccurrence(corpus, window_size)\n",
    "weightingdic = {(w1, w2): weighting(w1, w2, cooc) for (w1, w2) in cooc}\n",
    "print(f\"Co-occurrence pairs: {len(cooc)}\")\n",
    "\n",
    "emb_size = 100\n",
    "batch_size = 512 # Increase batch_size to prevent high varience\n",
    "num_epochs = 2500\n",
    "\n",
    "model_glove = GloVeModel(voc_size, emb_size).to(device)\n",
    "optimizer_glove = optim.Adam(model_glove.parameters(), lr=0.005) # Might reduce learning rate to prevent overshoot minimum\n",
    "\n",
    "print(\"Training GloVe...\")\n",
    "for epoch in range(num_epochs):\n",
    "    input_b, target_b, cooc_b, weight_b = randombatch_glove(batch_size, cooc, weightingdic)\n",
    "    input_b = torch.from_numpy(input_b).to(device)\n",
    "    target_b = torch.from_numpy(target_b).to(device)\n",
    "    cooc_b = torch.from_numpy(cooc_b).to(device)\n",
    "    weight_b = torch.from_numpy(weight_b).to(device)\n",
    "\n",
    "    optimizer_glove.zero_grad()\n",
    "    loss = model_glove(input_b.unsqueeze(1), target_b.unsqueeze(1), cooc_b.unsqueeze(1), weight_b.unsqueeze(1))\n",
    "    loss.backward()\n",
    "    optimizer_glove.step()\n",
    "\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"GloVe Epoch {epoch+1}, loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training completed! Both Word2Vec (NS) and GloVe ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64044ebe",
   "metadata": {},
   "source": [
    "### **TASK 2: Model Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cebc22e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Table 1: Training Results (Window=2)\n",
      "| Model | Window Size | Training Loss | Training Time (s) |\n",
      "|-------|-------------|---------------|-------------------|\n",
      "| Skip-gram (NS) | 2 | 7.95 | 56.6 |\n",
      "| GloVe | 2 | 4595.22 | 14.4 |\n"
     ]
    }
   ],
   "source": [
    "# Training Loss/Time Table\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ‡∏ß‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ó‡∏£‡∏ô (‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÄ‡∏ó‡∏£‡∏ô‡πÉ‡∏´‡∏°‡πà ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á)\n",
    "def train_and_time(model_class, train_fn, name, epochs=1000):\n",
    "    start_time = time.time()\n",
    "    model = model_class(voc_size, emb_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # train_fn() = ‡∏™‡∏∏‡πà‡∏° batch + train step\n",
    "        loss = train_fn(model, optimizer)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    final_loss = loss.item()\n",
    "    return model, final_loss, train_time\n",
    "\n",
    "# Skip-gram NS (‡∏à‡∏≤‡∏Å Task 1)\n",
    "def ns_train_step(model, optimizer):\n",
    "    centers, positives, negatives = randombatch_with_neg(batch_size, skipgrams, neg_sampling_probs)\n",
    "    centers, positives, negatives = centers.to(device), positives.to(device), negatives.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(centers, positives, negatives)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "# GloVe train step (‡∏à‡∏≤‡∏Å Task 1)\n",
    "def glove_train_step(model, optimizer):\n",
    "    input_b, target_b, cooc_b, weight_b = randombatch_glove(batch_size, cooc, weightingdic)\n",
    "    input_b = torch.LongTensor(input_b).unsqueeze(1).to(device)\n",
    "    target_b = torch.LongTensor(target_b).unsqueeze(1).to(device)\n",
    "    cooc_b = torch.FloatTensor(cooc_b).unsqueeze(1).to(device)\n",
    "    weight_b = torch.FloatTensor(weight_b).unsqueeze(1).to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_b, target_b, cooc_b, weight_b)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "# Table 1: Training Loss/Time (window_size=2)\n",
    "results = {\n",
    "    'Skip-gram (NS)': train_and_time(SkipgramNegSampling, ns_train_step, 'Skip-gram (NS)'),\n",
    "    'GloVe': train_and_time(GloVeModel, glove_train_step, 'GloVe')\n",
    "}\n",
    "\n",
    "print(\"## Table 1: Training Results (Window=2)\")\n",
    "print(\"| Model | Window Size | Training Loss | Training Time (s) |\")\n",
    "print(\"|-------|-------------|---------------|-------------------|\")\n",
    "for name, (model, loss, time_s) in results.items():\n",
    "    print(f\"| {name} | 2 | {loss:.2f} | {time_s:.1f} |\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95c7802c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntactic: 0 questions\n",
      "Semantic: 0 questions\n",
      "\n",
      "## Table 2: Intrinsic Evaluation (Google Analogy)\n",
      "| Model | Window Size | Syntactic Acc (%) | Semantic Acc (%) |\n",
      "|-------|-------------|-------------------|------------------|\n",
      "| Skip-gram (NS) | 2 | 0.00 | 0.00 |\n",
      "| GloVe | 2 | 0.00 | 0.00 |\n"
     ]
    }
   ],
   "source": [
    "# Intrinsic Eval: Syntactic + Semantic Accuracy (Google Analogy Test Set)\n",
    "\n",
    "# Download Google analogy dataset\n",
    "import requests\n",
    "url = \"https://github.com/tomsercu/lstm/raw/master/data/questions-words.txt\"\n",
    "response = requests.get(url)\n",
    "lines = response.text.splitlines()\n",
    "\n",
    "def parse_analogies(lines):\n",
    "    syntactic, semantic = [], []\n",
    "    current_category = None  # ‡πÄ‡∏Å‡πá‡∏ö category ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(':'):\n",
    "            current_category = line[2:]  # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó category\n",
    "            continue\n",
    "        if line and current_category:  # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥ + category\n",
    "            words = line.split()\n",
    "            if len(words) == 4:\n",
    "                if 'gram' in current_category:  # syntactic categories\n",
    "                    syntactic.append(words)\n",
    "                elif 'cat' in current_category:  # semantic categories\n",
    "                    semantic.append(words)\n",
    "    \n",
    "    return syntactic[:5000], semantic[:5000]\n",
    "\n",
    "syntactic, semantic = parse_analogies(lines)\n",
    "print(f\"Syntactic: {len(syntactic)} questions\")\n",
    "print(f\"Semantic: {len(semantic)} questions\")\n",
    "\n",
    "\n",
    "def get_embedding(model, word):\n",
    "    \"\"\"‡∏î‡∏∂‡∏á embedding ‡∏à‡∏≤‡∏Å model\"\"\"\n",
    "    if word not in word2index:\n",
    "        return None\n",
    "    idx = word2index[word]\n",
    "    with torch.no_grad():\n",
    "        v_emb = model.v_embed(torch.tensor([idx]).to(device))\n",
    "        if hasattr(model, 'u_embed'):\n",
    "            u_emb = model.u_embed(torch.tensor([idx]).to(device))\n",
    "            emb = (v_emb + u_emb) / 2\n",
    "        else:\n",
    "            emb = v_emb\n",
    "        return emb.cpu().numpy().squeeze()\n",
    "\n",
    "def analogy_accuracy(model, analogies):\n",
    "    \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy: a:b :: c:? ‚Üí argmax_d cos(b-a+c, d)\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for a, b, c, d in analogies:\n",
    "        emb_a = get_embedding(model, a)\n",
    "        emb_b = get_embedding(model, b)\n",
    "        emb_c = get_embedding(model, c)\n",
    "        emb_d = get_embedding(model, d)\n",
    "        \n",
    "        if emb_a is None or emb_b is None or emb_c is None or emb_d is None:\n",
    "            continue\n",
    "            \n",
    "        # Vector: b - a + c\n",
    "        vec = emb_b - emb_a + emb_c\n",
    "        \n",
    "        # ‡∏´‡∏≤ word ‡∏ó‡∏µ‡πà cos sim ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° a,b,c)\n",
    "        best_word = None\n",
    "        best_sim = -1\n",
    "        for word in vocab[:5000]:  # test 5000 ‡∏Ñ‡∏≥‡πÄ‡∏£‡πá‡∏ß\n",
    "            if word in [a, b, c]:\n",
    "                continue\n",
    "            emb_w = get_embedding(model, word)\n",
    "            if emb_w is not None:\n",
    "                sim = np.dot(vec, emb_w) / (np.linalg.norm(vec) * np.linalg.norm(emb_w))\n",
    "                if sim > best_sim:\n",
    "                    best_sim = sim\n",
    "                    best_word = word\n",
    "        \n",
    "        if best_word == d:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    return correct / total * 100 if total > 0 else 0\n",
    "\n",
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy\n",
    "syn_ns = analogy_accuracy(model_ns, syntactic)\n",
    "sem_ns = analogy_accuracy(model_ns, semantic)\n",
    "syn_glove = analogy_accuracy(model_glove, syntactic)\n",
    "sem_glove = analogy_accuracy(model_glove, semantic)\n",
    "\n",
    "print(\"\\n## Table 2: Intrinsic Evaluation (Google Analogy)\")\n",
    "print(\"| Model | Window Size | Syntactic Acc (%) | Semantic Acc (%) |\")\n",
    "print(\"|-------|-------------|-------------------|------------------|\")\n",
    "print(f\"| Skip-gram (NS) | 2 | {syn_ns:.2f} | {sem_ns:.2f} |\")\n",
    "print(f\"| GloVe | 2 | {syn_glove:.2f} | {sem_glove:.2f} |\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5de5e52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 353 WordSim-353 pairs from combined.csv\n",
      "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: [('love', 'sex', 6.77), ('tiger', 'cat', 7.35), ('tiger', 'tiger', 10.0)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# üìÅ ‡πÇ‡∏´‡∏•‡∏î WordSim-353 ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå local \n",
    "wsim_df = pd.read_csv(\"../dataset/combined.csv\")  # ‡∏£‡∏±‡∏ô‡πÉ‡∏ô folder ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå\n",
    "wsim_data = []\n",
    "for _, row in wsim_df.iterrows():\n",
    "    w1, w2, score = row['Word 1'].lower(), row['Word 2'].lower(), float(row['Human (mean)'])\n",
    "    wsim_data.append((w1, w2, score))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(wsim_data)} WordSim-353 pairs from combined.csv\")\n",
    "print(f\"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {wsim_data[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8df2bcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Table 3: WordSim-353 Spearman (combined.csv)\n",
      "| Model          | Spearman Corr | Valid Pairs |\n",
      "|----------------|---------------|-------------|\n",
      "| Skip-gram (NS) | -0.061 | 181 |\n",
      "| GloVe          | 0.131 | 181 |\n"
     ]
    }
   ],
   "source": [
    "# Spearman correlation function (‡πÉ‡∏ä‡πâ get_embedding ‡∏à‡∏≤‡∏Å Task 2)\n",
    "def spearman_correlation(model, word_pairs):\n",
    "    model_sims = []\n",
    "    human_sims = []\n",
    "    for w1, w2, human_sim in word_pairs:\n",
    "        emb1 = get_embedding(model, w1)\n",
    "        emb2 = get_embedding(model, w2)\n",
    "        if emb1 is not None and emb2 is not None:\n",
    "            sim = np.dot(emb1, emb2)  # ‡πÉ‡∏ä‡πâ dot product ‡∏ï‡∏≤‡∏°‡πÇ‡∏à‡∏ó‡∏¢‡πå\n",
    "            model_sims.append(sim)\n",
    "            human_sims.append(human_sim)\n",
    "\n",
    "    valid_pairs = len(model_sims)\n",
    "    if valid_pairs < 2:\n",
    "        return 0.0, valid_pairs\n",
    "    corr = stats.spearmanr(model_sims, human_sims)[0]\n",
    "    return corr, valid_pairs\n",
    "\n",
    "\n",
    "# ‡∏£‡∏±‡∏ô correlation (‡πÉ‡∏ä‡πâ model_ns, model_glove ‡∏à‡∏≤‡∏Å Task 1)\n",
    "corr_ns, n_ns = spearman_correlation(model_ns, wsim_data)\n",
    "corr_glove, n_glove = spearman_correlation(model_glove, wsim_data)\n",
    "\n",
    "print(\"\\n## Table 3: WordSim-353 Spearman (combined.csv)\")\n",
    "print(\"| Model          | Spearman Corr | Valid Pairs |\")\n",
    "print(\"|----------------|---------------|-------------|\")\n",
    "print(f\"| Skip-gram (NS) | {corr_ns:.3f} | {n_ns} |\")\n",
    "print(f\"| GloVe          | {corr_glove:.3f} | {n_glove} |\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ccf722",
   "metadata": {},
   "source": [
    "### **2.3 Word Similarity Evaluation**\n",
    "\n",
    "- For the similarity evaluation, the WordSim-353 dataset (combined.csv) was used as the gold-standard similarity resource. For each word pair \\((w_1, w_2)\\) in the dataset, the similarity predicted by each model was computed as the **dot product** between the corresponding word embeddings. The human similarity scores provided in WordSim-353 were then compared with the model-predicted similarities using the **Spearman rank correlation coefficient** implemented in `scipy.stats.spearmanr`.  \n",
    "\n",
    "- In this setup, the Skip-gram with negative sampling model achieved a Spearman correlation of approximately **-0.10**, while the GloVe model achieved a correlation of approximately **0.01**, based on **181 word pairs** for which both words were present in the models‚Äô vocabularies. These low correlation values indicate that, with the relatively small training corpus used in this assignment, the learned embeddings do **not yet capture human-like word similarity judgments effectively**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccdaa49",
   "metadata": {},
   "source": [
    "### **Export Models, Vocabs, Context Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e03a9d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: skipgram_ns.pt\n",
      "Saved: glove_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save Skip-gram (NEG)\n",
    "MODEL_WORD2VEC = \"skipgram_ns.pt\"\n",
    "torch.save(model_ns.state_dict(), MODEL_WORD2VEC)\n",
    "print(\"Saved:\", MODEL_WORD2VEC)\n",
    "\n",
    "# Save GloVe\n",
    "MODEL_GLOVE = \"glove_model.pt\"\n",
    "torch.save(model_glove.state_dict(), MODEL_GLOVE)\n",
    "print(\"Saved:\", MODEL_GLOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18c53cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2index, vocab using pickle \n",
    "import pickle\n",
    "\n",
    "with open(\"word2index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word2index, f)\n",
    "\n",
    "with open(\"index2word.pkl\", \"wb\") as f:\n",
    "    pickle.dump(index2word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d31b4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 10007\n",
      "Saved corpus_sentences.pkl\n"
     ]
    }
   ],
   "source": [
    "# corpus sentenses\n",
    "# corpus ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ List[List[str]] ‡∏à‡∏≤‡∏Å load_reuters_corpus()\n",
    "\n",
    "print(type(corpus), len(corpus))  # ‡πÄ‡∏ä‡πá‡∏Å‡πÄ‡∏•‡πà‡∏ô ‡πÜ\n",
    "\n",
    "with open(\"corpus_sentences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(corpus, f)\n",
    "\n",
    "print(\"Saved corpus_sentences.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff4dabe",
   "metadata": {},
   "source": [
    "#### `‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏à‡∏° syntax ‡πÑ‡∏õ‡∏ù‡∏∂‡∏Å‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b85bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
